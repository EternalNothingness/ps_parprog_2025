\documentclass[parskip]{scrartcl}

\usepackage{multirow}
\usepackage{minted}

\subject{Sheet 03}
\title{PS Parallel Programming}
\author{Patrick Wintner}
\date{\today}

\begin{document}
	\maketitle
	
	\section{Mandelbrot}
	The execution time of the program mandelbrot is measured.
	\subsection{Source Code}
	\inputminted{c}{mandelbrot/mandelbrot.c}
	\subsection{Measurement Method}
	The measurement was done on the LCC3 cluster by calling sbatch job.sh. The following scripts are involved in the measurement process.
	\subsubsection{SLURM Job Script}
	\inputminted{bash}{mandelbrot/job.sh}
	\subsubsection{Main Script}
	\inputminted{bash}{mandelbrot/main.sh}
	
	The measurement results are stored in mandelbrot\_measurments.log, which is read by process\_results to compute the average execution time and standard deviation, which are stored in mandelbrot\_processed.log.
	
	\subsection{Measurement Results}
	\begin{tabular}{|c|c|c|}
		\hline time/s & mean/s & standard deviation/s\\
		\hline 17.77 & \multirow{3}*{17.74} & \multirow{3}*{0.0265}\\
		\cline{1-1} 17.72 & &\\
		\cline{1-1} 17.73 & &\\
		\hline
	\end{tabular}
	
	\subsection{Suggestions for performance improvement and parallelisation}
	The calculations of the colour of different pixels are independent, therefore those calculations could be done parallel.
	
	\section{False Sharing}
	The execution time of two versions of the program false\_sharing are compared, followed by a more detailed analysis using perf.
	\subsection{Source Code}
	\subsubsection{Version 1}
	\inputminted{c}{false_sharing/false_sharing.c}
	\subsubsection{Version 2}
	\inputminted{c}{false_sharing/false_sharing_2.c}
	\subsubsection{Differences and Implications}
	Both versions allocate storage dynamically to a pointer called sum. Each thread is responsible for incrementing exactly one value of the allocated memory (therefore they should not influence each other), until it equals the value given as command line parameter. The final result is the sum of all elements.
	
	In the first version, there is no padding between the memory locations used by the threads for the computation. This means that it is likely that several memory locations used by different threads will be stored in the same cache line. If a thread writes to a memory location in a cache line, it invalidates also all other data stored in the same cache line. Therefore if another thread wants to increment another value stored in the same cache line, the previous thread has to write the cache line back into memory, causing significant delay (even though otherwise the threads are independent and the values are stored in the cache!).
	
	In the second version, there is padding between the memory locations used for incrementing, hopefully preventing that memory locations of different threads are getting loaded into the same cache line.
	
	\subsection{Measurement Method}
	The measurement was done on the LCC3 cluster by calling ./execall. The following scripts are involved in the measurement process.
	\subsubsection{Execall Script}
	\inputminted{bash}{false_sharing/execall.sh}
	
	The first job measures the execution time of both versions with six threads on either one or evenly distributed on two processors. The second job uses perf to get an high-level overview of events happening during execution, while the third looks specifically at cache events.
	\subsubsection{SLURM Job Script}
	\inputminted{bash}{false_sharing/job.sh}
	\subsubsection{Main Script}
	\inputminted{bash}{false_sharing/main.sh}
	\subsection{Measurement Results}
	\subsubsection{execution time}
	\begin{tabular}{|c|c|c|}
		\hline \# processors & 1 & 2\\
		\hline $t_{false\_sharing\_1}/s$ & 3.14e-1 & 3.91e-1\\
		\hline $t_{false\_sharing\_2}/s$ & 2.07e-1 & 2.07e-1\\
		\hline
	\end{tabular}
	
	The second version is not affected by increasing the number of processors, while the first suffers an increase in execution time and is generally slower. The reasons why the first version is slower are stated above. Considering that communication between cores of different processors is likely slower than that of cores on the same processor and that there are likely lots of write-back requests when running the first version, it is not surprising that distributing the threads on different processors increases the execution time.
	
	\subsubsection{Perf - Overview}
	
	The following data was measured with all six threads on one processor.
	{\footnotesize
	\begin{verbatim}
Performance counter stats for './false_sharing 100000000':

          1,812.40 msec task-clock:u              #    5.648 CPUs utilized          
                 0      context-switches:u        #    0.000 /sec                   
                 0      cpu-migrations:u          #    0.000 /sec                   
                81      page-faults:u             #   44.692 /sec                   
     5,265,442,510      cycles:u                  #    2.905 GHz                      (83.39%)
     4,072,468,552      stalled-cycles-frontend:u #   77.34% frontend cycles idle     (83.18%)
       947,310,743      stalled-cycles-backend:u  #   17.99% backend cycles idle      (66.64%)
     2,413,081,608      instructions:u            #    0.46  insn per cycle         
                                                  #    1.69  stalled cycles per insn  (83.40%)
       604,385,457      branches:u                #  333.473 M/sec                    (83.45%)
             7,127      branch-misses:u           #    0.00% of all branches          (83.44%)

       0.320876265 seconds time elapsed

       1.796202000 seconds user
       0.001986000 seconds sys

Performance counter stats for './false_sharing_2 100000000':

          1,250.96 msec task-clock:u              #    5.817 CPUs utilized          
                 0      context-switches:u        #    0.000 /sec                   
                 0      cpu-migrations:u          #    0.000 /sec                   
                80      page-faults:u             #   63.951 /sec                   
     3,631,302,842      cycles:u                  #    2.903 GHz                      (83.29%)
     2,439,312,866      stalled-cycles-frontend:u #   67.17% frontend cycles idle     (83.22%)
       593,658,873      stalled-cycles-backend:u  #   16.35% backend cycles idle      (66.43%)
     2,411,063,345      instructions:u            #    0.66  insn per cycle         
                                                  #    1.01  stalled cycles per insn  (83.28%)
       603,202,626      branches:u                #  482.193 M/sec                    (83.48%)
               435      branch-misses:u           #    0.00% of all branches          (83.58%)

       0.215052654 seconds time elapsed
	\end{verbatim}
 	}
  	The following data was measured with six threads distributed over two processors.
  	{\footnotesize
  	\begin{verbatim}     
Performance counter stats for './false_sharing 100000000':

          2,217.21 msec task-clock:u              #    5.460 CPUs utilized          
                 0      context-switches:u        #    0.000 /sec                   
                 0      cpu-migrations:u          #    0.000 /sec                   
                81      page-faults:u             #   36.532 /sec                   
     6,444,167,778      cycles:u                  #    2.906 GHz                      (83.29%)
     5,253,540,958      stalled-cycles-frontend:u #   81.52% frontend cycles idle     (83.31%)
     1,537,181,968      stalled-cycles-backend:u  #   23.85% backend cycles idle      (66.72%)
     2,414,069,616      instructions:u            #    0.37  insn per cycle         
                                                  #    2.18  stalled cycles per insn  (83.36%)
       603,408,765      branches:u                #  272.148 M/sec                    (83.36%)
             6,968      branch-misses:u           #    0.00% of all branches          (83.45%)

       0.406083446 seconds time elapsed
       
Performance counter stats for './false_sharing_2 100000000':

          1,242.99 msec task-clock:u              #    5.881 CPUs utilized          
                 0      context-switches:u        #    0.000 /sec                   
                 0      cpu-migrations:u          #    0.000 /sec                   
                81      page-faults:u             #   65.165 /sec                   
     3,608,890,769      cycles:u                  #    2.903 GHz                      (83.11%)
     2,421,206,931      stalled-cycles-frontend:u #   67.09% frontend cycles idle     (83.11%)
       635,247,518      stalled-cycles-backend:u  #   17.60% backend cycles idle      (67.09%)
     2,402,511,506      instructions:u            #    0.67  insn per cycle         
                                                  #    1.01  stalled cycles per insn  (83.59%)
       600,892,351      branches:u                #  483.424 M/sec                    (83.59%)
               400      branch-misses:u           #    0.00% of all branches          (83.19%)

       0.211362765 seconds time elapsed
	\end{verbatim}
	}
	The number of processors does not have a significant impact on the second version.  The number of clock cycles and branch misses of the first version are significant higher than those of the second version. Increasing the number of processors increases the number of clock cycles for the first version.
	
	\subsubsection{Perf - Details}
	The following data was measured with all six threads on one processor.
	{\footnotesize
	\begin{verbatim}
Performance counter stats for './false_sharing 100000000':

               507      LLC-load-misses:u                                           
               163      LLC-store-misses:u                                          

       0.318045312 seconds time elapsed

       1.791884000 seconds user
       0.000994000 seconds sys

 Performance counter stats for './false_sharing_2 100000000':

               146      LLC-load-misses:u                                           
                58      LLC-store-misses:u                                          

       0.212332630 seconds time elapsed

       1.232476000 seconds user
       0.002983000 seconds sys
	\end{verbatim}
	}
  	The following data was measured with six threads distributed over two processors.
  	{\footnotesize
  	\begin{verbatim}
Performance counter stats for './false_sharing 100000000':

         1,153,111      LLC-load-misses:u                                           
         1,452,548      LLC-store-misses:u                                          

       0.402423131 seconds time elapsed

       2.198254000 seconds user
       0.001989000 seconds sys
       
Performance counter stats for './false_sharing_2 100000000':

               250      LLC-load-misses:u                                           
                86      LLC-store-misses:u                                          

       0.209930705 seconds time elapsed

       1.235174000 seconds user
       0.001980000 seconds sys
  	\end{verbatim}
	The number of LLC-load-misses of the first version is signifiantly higher, especially when the threads are distributed over two processors.
\end{document}